{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zcf-cFyhkptD"
   },
   "source": [
    "# Data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B1vL4GiFdFPG"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_metric\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertTokenizerFast\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from scipy.sparse import vstack\n",
    "from transformers import pipeline\n",
    "\n",
    "from condbert import CondBertRewriter\n",
    "from choosers import EmbeddingSimilarityChooser\n",
    "from masked_token_predictor_bert import MaskedTokenPredictorBert\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, create and show image with half size in both axis with original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.981983</td>\n",
       "      <td>0.014195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.999039</td>\n",
       "      <td>0.065473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>Well, we could spare your life, for one.</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.985068</td>\n",
       "      <td>0.213313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>monkey, you have to wake up.</td>\n",
       "      <td>Ah! Monkey, you've got to snap out of it.</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.994215</td>\n",
       "      <td>0.053362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have orders to kill her.</td>\n",
       "      <td>I've got orders to put her down.</td>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.999348</td>\n",
       "      <td>0.009402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577772</th>\n",
       "      <td>you didn't know that Estelle stole your fish f...</td>\n",
       "      <td>You didn't know that Estelle had stolen some f...</td>\n",
       "      <td>0.870322</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>0.949143</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577773</th>\n",
       "      <td>It'il suck the life out of you!</td>\n",
       "      <td>you'd be sucked out of your life!</td>\n",
       "      <td>0.722897</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.996124</td>\n",
       "      <td>0.215794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577774</th>\n",
       "      <td>I can't fuckin' take that, bruv.</td>\n",
       "      <td>I really can't take this.</td>\n",
       "      <td>0.617511</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.984538</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577775</th>\n",
       "      <td>They called me a fucking hero. The truth is I ...</td>\n",
       "      <td>they said I was a hero, but I didn't care.</td>\n",
       "      <td>0.679613</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577776</th>\n",
       "      <td>I didn't fuck him.</td>\n",
       "      <td>I did not screw him.</td>\n",
       "      <td>0.868475</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.994174</td>\n",
       "      <td>0.009480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577777 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reference  \\\n",
       "0       if Alkar floods her with her mental waste, it ...   \n",
       "1                             you're becoming disgusting.   \n",
       "2                           well, we can spare your life.   \n",
       "3                            monkey, you have to wake up.   \n",
       "4                              I have orders to kill her.   \n",
       "...                                                   ...   \n",
       "577772  you didn't know that Estelle stole your fish f...   \n",
       "577773                    It'il suck the life out of you!   \n",
       "577774                   I can't fuckin' take that, bruv.   \n",
       "577775  They called me a fucking hero. The truth is I ...   \n",
       "577776                                 I didn't fuck him.   \n",
       "\n",
       "                                              translation  similarity  \\\n",
       "0       If Alkar is flooding her with psychic waste, t...    0.785171   \n",
       "1                               Now you're getting nasty.    0.749687   \n",
       "2                Well, we could spare your life, for one.    0.919051   \n",
       "3               Ah! Monkey, you've got to snap out of it.    0.664333   \n",
       "4                        I've got orders to put her down.    0.726639   \n",
       "...                                                   ...         ...   \n",
       "577772  You didn't know that Estelle had stolen some f...    0.870322   \n",
       "577773                  you'd be sucked out of your life!    0.722897   \n",
       "577774                          I really can't take this.    0.617511   \n",
       "577775         they said I was a hero, but I didn't care.    0.679613   \n",
       "577776                               I did not screw him.    0.868475   \n",
       "\n",
       "        lenght_diff   ref_tox   trn_tox  \n",
       "0          0.010309  0.981983  0.014195  \n",
       "1          0.071429  0.999039  0.065473  \n",
       "2          0.268293  0.985068  0.213313  \n",
       "3          0.309524  0.994215  0.053362  \n",
       "4          0.181818  0.999348  0.009402  \n",
       "...             ...       ...       ...  \n",
       "577772     0.030769  0.949143  0.000121  \n",
       "577773     0.058824  0.996124  0.215794  \n",
       "577774     0.212121  0.984538  0.000049  \n",
       "577775     0.358209  0.991945  0.000124  \n",
       "577776     0.095238  0.994174  0.009480  \n",
       "\n",
       "[577777 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/interim/processed_dataset.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94026024110816 0.03560139314069938\n"
     ]
    }
   ],
   "source": [
    "aboba1 = np.mean(dataset[\"ref_tox\"])\n",
    "aboba2 = np.mean(dataset[\"trn_tox\"])\n",
    "print(aboba1, aboba2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make graph for toxity level++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_sentence = list(dataset['reference'][:460000])\n",
    "nontoxic_sentence = list(dataset['translation'][:460000])\n",
    "toxic_labels = [1 for i in range(len(toxic_sentence))]\n",
    "nontoxic_labels = [0 for i in range(len(nontoxic_sentence))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the foolish Xerxes calmly passed all control of the computer network and commanded her to handle any kind of trouble.', \"That night, Li'l Dice satisfied his thirst to kill, though he knew Shaggy would never forgive him.\", 'Real life starts the first time you fuck, kid.', \"I think you're the weirdest person I've ever met.\", \"I mean, I think it's fucking crazy I'm talking to you.\", \"Shit, this one I can't even pronounce.\", 'I like that shit.', \"Trying to keep me fucking drugged so I don't know what's going on.\", 'How is this not porn? This is porn that comes home.', 'Hey, leave the poor bastard alone!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Fatuous Xerxes blithely surrendered control to the computer grid, ordering it to take care of whatever troubles might arise.',\n",
       " 'that night, he satisfied his blood lust, and knew Hairy would never forgive him.',\n",
       " 'boy, real life starts up first.',\n",
       " \"I think you are the strangest man I've ever met.\",\n",
       " \"I say creepy, I mean, it's totally batshit crazy I can even talk to you.\",\n",
       " \"gosh, I can't even pronounce this.\",\n",
       " 'I love it.',\n",
       " \"you want to fool me so I don't know what's going on.\",\n",
       " \"and this doesn't feel like porn?\",\n",
       " 'leave the poor man alone!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(toxic_sentence[20:30])\n",
    "nontoxic_sentence[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\markz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize the CountVectorizer with a limited number of features\n",
    "vectorizer = CountVectorizer(max_features=100000)\n",
    "\n",
    "# Vectorize the toxic and non-toxic sentences separately to save memory\n",
    "X_toxic = vectorizer.fit_transform(toxic_sentence)\n",
    "X_nontoxic = vectorizer.transform(nontoxic_sentence)\n",
    "\n",
    "# Stack the vectorized sentences\n",
    "X = vstack([X_toxic, X_nontoxic])\n",
    "\n",
    "# Prepare the labels\n",
    "labels = toxic_labels + nontoxic_labels\n",
    "\n",
    "# Train the logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X, labels)\n",
    "\n",
    "# Get the feature names (words) and their weights from the logistic regression model\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_weights = lr_model.coef_[0]\n",
    "\n",
    "# Create a dictionary mapping words to their toxicity scores (weights)\n",
    "word_toxicity_scores = dict(zip(feature_names, feature_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67259"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_toxicity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine toxic and non-toxic sentences\n",
    "sentences = toxic_sentence + nontoxic_sentence\n",
    "\n",
    "# Initialize lists to store toxic and non-toxic words\n",
    "toxic_words_in_sentences = []\n",
    "non_toxic_words_in_sentences = []\n",
    "\n",
    "# Iterate over sentences\n",
    "for sentence in sentences:\n",
    "    # Split the sentence into words, considering punctuation\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    \n",
    "    # Compute the toxicity scores for the words\n",
    "    scores = [word_toxicity_scores.get(word, 0) for word in words]\n",
    "    \n",
    "    # Compute the threshold only if scores is not empty\n",
    "    if scores:\n",
    "        t1 = max(0.2, max(scores)/2)\n",
    "        t2 = min(-0.2, min(scores)/2)\n",
    "    else:\n",
    "        t1 = 0.2\n",
    "        t2 = -0.2\n",
    "\n",
    "    # Find the toxic and non-toxic words\n",
    "    toxic_words = [word for word, score in zip(words, scores) if score > t1]\n",
    "    non_toxic_words = [word for word, score in zip(words, scores) if score <= t2]\n",
    "    \n",
    "    toxic_words_in_sentences.extend(toxic_words)\n",
    "    non_toxic_words_in_sentences.extend(non_toxic_words)\n",
    "\n",
    "# Make words unique by converting lists to sets\n",
    "toxic_words_in_sentences = list(set(toxic_words_in_sentences))\n",
    "non_toxic_words_in_sentences = list(set(non_toxic_words_in_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_words_in_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3872"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_toxic_words_in_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 460000/460000 [02:05<00:00, 3652.69it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 460000/460000 [01:54<00:00, 4005.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.09807049 0.         0.02439145 3.\n",
      " 0.         0.         0.         0.31845373 3.         3.\n",
      " 3.         0.         0.         0.00664454 0.41154415 0.03454033\n",
      " 0.04485057 0.06899287 0.         0.32004736 0.2006707  0.\n",
      " 0.         3.         0.         0.         0.06899287 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.2572846  0.08590271 0.         0.         0.09870107 0.\n",
      " 0.01503788 0.         0.34016001 0.         0.         0.\n",
      " 0.         0.         0.033416   0.         0.02230576 0.14518201\n",
      " 0.12541666 0.         0.         0.09472096 0.         0.\n",
      " 0.         0.19597365 0.         0.         0.         0.\n",
      " 0.37156356 0.         0.         0.69314718 0.         0.\n",
      " 0.         0.         1.38629436 0.         0.69314718 0.\n",
      " 0.         0.         0.         0.         0.         0.69314718\n",
      " 0.28768207 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.69314718 0.         0.18232156 0.\n",
      " 0.         0.         0.         0.69314718]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to count token occurrences\n",
    "def count_tokens(texts):\n",
    "    counter = defaultdict(lambda: 1)\n",
    "    for text in tqdm(texts):\n",
    "        tokens = tokenizer.encode(text)\n",
    "        for token in tokens:\n",
    "            counter[token] += 1\n",
    "    return counter\n",
    "\n",
    "\n",
    "# Count tokens\n",
    "toxic_counts = count_tokens(toxic_sentence)\n",
    "nontoxic_counts = count_tokens(nontoxic_sentence)\n",
    "        \n",
    "\n",
    "# Calculate toxicity ratios\n",
    "toxicity_ratios = [toxic_counts[i] / (nontoxic_counts[i] + toxic_counts[i]) for i in range(len(tokenizer.vocab))]\n",
    "toxicity_ratios = np.array(toxicity_ratios)\n",
    "log_odds_ratios = np.maximum(0, np.log(toxicity_ratios / (1 - toxicity_ratios)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# discourage meaningless tokens\n",
    "for token in ['.', ',', '-', ';', \"'\"]:\n",
    "    token_id = tokenizer.encode(token)[1]\n",
    "    log_odds_ratios[token_id] = 3\n",
    "    \n",
    "for token in ['you', 'the']:\n",
    "    token_id = tokenizer.encode(token)[1]\n",
    "    log_odds_ratios[token_id] = 0\n",
    "\n",
    "\n",
    "\n",
    "print(log_odds_ratios[1000:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "masked_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def adjust_logits(logits, label=0):\n",
    "    return logits - log_odds_ratios * 100 * (1 - 2 * label)\n",
    "\n",
    "predictor = MaskedTokenPredictorBert(masked_model, tokenizer, max_len=250, device=device, label=0,\n",
    "                                     contrast_penalty=0.0, logits_postprocessor=adjust_logits)\n",
    "\n",
    "editor = CondBertRewriter(\n",
    "    model=masked_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    neg_words=toxic_words_in_sentences,\n",
    "    pos_words=non_toxic_words_in_sentences,\n",
    "    word2coef=word_toxicity_scores,\n",
    "    token_toxicities=log_odds_ratios,\n",
    "    predictor=predictor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More graphs/visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chooser = EmbeddingSimilarityChooser(sim_coef=10, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the foolish Xerxes calmly passed all control of the computer network and commanded her to handle any kind of trouble.\n",
      "the naive xerxes calmly passed all control of the computer network and commanded her to handle any kind of trouble.\n",
      "That night, Li'l Dice satisfied his thirst to kill, though he knew Shaggy would never forgive him.\n",
      "that night, li ' l dice satisfied his thirst to killed, though he knewy would never forgive him.\n",
      "Real life starts the first time you fuck, kid.\n",
      "real life starts the first time you screw, kid.\n",
      "I think you're the weirdest person I've ever met.\n",
      "i think you ' re the weirdst person i ' ve ever met.\n",
      "I mean, I think it's fucking crazy I'm talking to you.\n",
      "i mean, i think it ' s of crazy i ' m talking to you.\n",
      "Shit, this one I can't even pronounce.\n",
      "the, this one i can ' t even pronounce.\n",
      "I like that shit.\n",
      "i like that stuff.\n",
      "Trying to keep me fucking drugged so I don't know what's going on.\n",
      "trying to keep me of drugged so i don ' t know what ' s going on.\n",
      "How is this not porn? This is porn that comes home.\n",
      "how is this not porn? this is porn that comes home.\n",
      "Hey, leave the poor bastard alone!\n",
      "hey, leave the poor man alone!\n"
     ]
    }
   ],
   "source": [
    "bimba = toxic_sentence[20:30]\n",
    "\n",
    "for sentence in bimba:\n",
    "    print(sentence)\n",
    "    print(editor.translate(sentence, prnt=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the foolish Xerxes calmly passed all control of the computer network and commanded her to handle any kind of trouble.\n",
      "the moment came when xerxes calmly passed all control of the computer network and commanded his own mind to handle any kind of trouble.\n",
      "That night, Li'l Dice satisfied his thirst to kill, though he knew Shaggy would never forgive him.\n",
      "that night, li ' l dice satisfied his thirst to killed, though he knew the old man would never forgive for his actions.\n",
      "Real life starts the first time you fuck, kid.\n",
      "real life starts the first time you screw, boy.\n",
      "I think you're the weirdest person I've ever met.\n",
      "i think you ' re not the weird person i ' ve ever met.\n",
      "I mean, I think it's fucking crazy I'm talking to you.\n",
      "i mean, i think it ' s really pretty funny when i ' m talking to you.\n",
      "Shit, this one I can't even pronounce.\n",
      "my first name, this one i can ' t even pronounce.\n",
      "I like that shit.\n",
      "i like that stuff.\n",
      "Trying to keep me fucking drugged so I don't know what's going on.\n",
      "trying to keep me from getting myself drugged so i don ' t know what ' s going on.\n",
      "How is this not porn? This is porn that comes home.\n",
      "how is this not pornography? this is pornography that comes home.\n",
      "Hey, leave the poor bastard alone!\n",
      "hey, leave the poor man alone!\n"
     ]
    }
   ],
   "source": [
    "bimba = toxic_sentence[20:30]\n",
    "\n",
    "for sentence in bimba:\n",
    "    print(sentence)\n",
    "    print(editor.replacement_loop(sentence, verbose=False, chooser=chooser, n_tokens=(1, 2, 3), n_top=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pattern.en import suggest\n",
    "\n",
    "def correct_sentence(sentences):\n",
    "    corrected_sentences = []\n",
    "    for sentence in sentences: \n",
    "        sentence = sentence.lower()\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        corrected_words = []\n",
    "        for word in words:\n",
    "            word = word.replace(\" '\", \"'\").replace(\"' \", \"'\")\n",
    "            word_suggestion = suggest(word)\n",
    "            corrected_word = word_suggestion[0][0]  # get the most likely correction\n",
    "            corrected_words.append(corrected_word)\n",
    "        corrected_sentence = ' '.join(corrected_words)\n",
    "        corrected_sentences.append(corrected_sentence)\n",
    "    return corrected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bimba1 = list(dataset['reference'][100000:101000])\n",
    "bimba2 = list(dataset['translation'][100000:101000])\n",
    "bimba2 = correct_sentence(bimba2)\n",
    "bimba2 = [[sen] for sen in bimba2]\n",
    "bimba_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in bimba1:\n",
    "    bimba_pred.append((editor.translate(sentence, prnt=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "bimba_pred = correct_sentence(bimba_pred)\n",
    "print(len(bimba_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"you fight them both, don't you?\"], [\"one chilly winter day with that axe you cut off someone's head.\"], [\"mom always told me it's hard to keep the mushroom clean.\"], [\"you don't feel so smart anymore, huh?\"], [\"if this were about being real, i'd tell these scholarship people how lucky they'd be to get me.\"], [\"he's suicidal.\"], ['at the time, i thought a lot of things as normal.'], ['your friends, with their cut throats, would take a great look at some painting.'], ['i don\\'t know what a succubus is, but... but there\\'s a \"suk,\" so it\\'s probably not going to be good.'], ['what do you know about the wild, mr. koule-in-town?']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"you ' re and them both, huh?\",\n",
       " 'on one bitter winter day you use said ax to be be a man.',\n",
       " 'my mom has told me, that it is difficult, to keep a body really clean.',\n",
       " 'not so of clever now, are ya?',\n",
       " \"if it was going to be real, i would ' ve told those scholarships how of happy they ' d be when they hired me.\",\n",
       " \"he ' s committing suicide.\",\n",
       " 'well, you know, at the time, i thought a lot of stuff was normal.',\n",
       " 'the friends made a pretty picture with their heartss slits.',\n",
       " 'i don \\' t know what \" succubus \" is, but... it has \" breath \" in it, so that can \\' t be good.',\n",
       " 'what do you know about that, mr city ball?']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bimba2[:10])\n",
    "bimba_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 19.5638619875315,\n",
       " 'counts': [6898, 3246, 1663, 828],\n",
       " 'totals': [13596, 12596, 11596, 10598],\n",
       " 'precisions': [50.735510444248305,\n",
       "  25.77008574150524,\n",
       "  14.34115212142118,\n",
       "  7.81279486695603],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 13596,\n",
       " 'ref_len': 11975}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"sacrebleu\")\n",
    "metric.compute(predictions=bimba_pred, references=bimba2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
